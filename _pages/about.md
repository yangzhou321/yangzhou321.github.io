---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

My research interest includes neural network robustness and computer vision. I have published more than 4 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=kN8pdxcAAAAJ'>google scholar citations</a> <strong><span id='total_cit'>0</span></strong>. (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=kN8pdxcAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>).


# ğŸ”¥ News
- *2024.01*: &nbsp;ğŸ‰ğŸ‰ Paper accepted by TIP2024 !
- *2023.12*: &nbsp;ğŸ‰ğŸ‰ Paper accepted by AAAI2024 !
- *2023.03*: &nbsp;ğŸ‰ğŸ‰ Paper accepted by CVPR2023 !
- *2022.07*: &nbsp;ğŸ‰ğŸ‰ Paper accepted by ECCV2022 !

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/VQ.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Vector Quantization with Self-Attention for Quality-Independent Representation Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vector_Quantization_With_Self-Attention_for_Quality-Independent_Representation_Learning_CVPR_2023_paper.pdf)

**Zhou Yang**, Weisheng Dong*, Xin Li, Mengluan Huang, Yulin Sun and Guangming Shi

<strong><span class='show_paper_citations' data='kN8pdxcAAAAJ:d1gkVwhDpl0C'></span></strong> [**Project**](https://see.xidian.edu.cn/faculty/wsdong/Projects/VQSA.htm)

**Abstract:** Inspired by sparse representation in image restoration, we opt to address the degraded image recognition problem by learning image-quality-independent feature representation in a simple plug-and-play manner, that is, to introduce discrete vector quantization (VQ) to remove redundancy in recognition models.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2022</div><img src='images/UEM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Self-feature Distillation with Uncertainty Modeling for Degraded Image Recognition](https://link.springer.com/chapter/10.1007/978-3-031-20053-3_32)

**Zhou Yang**, Weisheng Dong*, Xin Li, Jinjian Wu, Leida Li and Guangming Shi 

<strong><span class='show_paper_citations' data='kN8pdxcAAAAJ:u-x6o8ySG0sC'></span></strong> [**Project**](https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=kN8pdxcAAAAJ&citation_for_view=kN8pdxcAAAAJ:u-x6o8ySG0sC)

**Abstract:** In standard feature distillation, Mean Squared Error, MSE treats each pixel in the feature equally and may result in relatively poor reconstruction performance in some difficult regions. To address this issue, we propose a novel self-feature distillation method with uncertainty modeling for better producing HQ-like features from low-quality observations in this paper.

</div>
</div>

# ğŸ“½ Projects

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BrickPal</div><img src='images/BrickPal.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

BrickPal: Assembly as Language, Immersive and Gamification, In-situ Creation


Xiaofeng Zhang, **Zhou Yang**, Xiao Tang, Yao Shi, Hongni Ye, Yi Wu and Ran Zhang

[**Video**](https://www.youtube.com/watch?v=-BBnm8yBUSg&t=1s)

**Introduction:** We have developed an application using AR glasses for assisting assembly on the Unity Vuforia platform. This program also utilizes NLP technology to predict the next assembly step and supports user-defined assembly requirements. My main contribution is the assembly guidance using AR technology.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">HCI Game</div><img src='images/Kinect.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

A Gesture-Based Human-Computer Interaction Shooting Game

**Zhou Yang**

[**Video**](https://www.youtube.com/watch?v=VxkAW67TZR8)

**Introduction:** Through combining a depth camera Kinect with object detection and gesture recognition algorithms, we can control the movement and shooting of a game characters based on the position and gestures of the hands in 3D space. Completed independently.
</div>
</div>



# ğŸ– Honors and Awards
- *2024.01* . Gold medal and ranked 6th / 1326 on Kaggle competition: [UBC Ovarian Cancer Subtype Classification and Outlier Detection (UBC-OCEAN)](https://www.kaggle.com/xdu4yangzhou). The main purpose is to help enhance the applicability and accessibility of accurate ovarian cancer diagnoses.
- *2022.06* . Ranked 9th on CVPR2022 workshop: Robust Models Towards Open-world Classification. Completed independently.
- *2021.08* . Ranked 3rd on DeeCamp 2023 (The competition is organized by Sinovation Ventures, and 01.AI is incubated by it.)


# ğŸ“– Educations
- *2019.09 - now*, PhD student at the School of Artificial and Intelligence, Xidian University, Xi'an, Shan Xi, China.
- *2015.09 - 2019.06*, Undergraduate student at the Advance Material and Nano Technology, Xidian University, Xi'an, Shan Xi, China.


<!--
# ğŸ’¬ Invited Talks
- *2021.06*, ---

# ğŸ’» Internships
- *2019.05 - 2020.02*, ---
-->
